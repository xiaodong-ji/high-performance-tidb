# 任务描述
- 分值：300
- 题目描述：
使用 sysbench、go-ycsb 和 go-tpc 分别对 TiDB 进行测试并且产出测试报告。

- 测试报告需要包括以下内容：
```
* 部署环境的机器配置(CPU、内存、磁盘规格型号)，拓扑结构(TiDB、TiKV 各部署于哪些节点)
* 调整过后的 TiDB 和 TiKV 配置
* 测试输出结果
* 关键指标的监控截图
  * TiDB Query Summary 中的 qps 与 duration
  * TiKV Details 面板中 Cluster 中各 server 的 CPU 以及 QPS 指标
  * TiKV Details 面板中 grpc 的 qps 以及 duration

输出：写出你对该配置与拓扑环境和 workload 下 TiDB 集群负载的分析，提出你认为的 TiDB 的性能的瓶颈所在(能提出大致在哪个模块即可)
```
- 截止时间：下周二（8.25）24:00:00(逾期提交不给分)


# 任务过程
### 环境描述
| 组件 | CPU（核）| 内存（GB）| 存储类型 | 网络 | 操作系统 | IP 地址 |
| :-----:| :----: | :----: | :----: | :----: | :----: | :----: |
| TiDB | 4 | 8 | SAS | 千兆网卡 | CentOS 7.5 | 192.168.0.19 |
| PD | 4 | 8 | SAS | 千兆网卡 | CentOS 7.5 | 192.168.0.19 |
| PD | 4 | 8 | SAS | 千兆网卡 | CentOS 7.5 | 192.168.0.18 |
| PD | 4 | 8 | SAS | 千兆网卡 | CentOS 7.5 | 192.168.0.17 |
| TiKV | 8 | 8 | SAS | 千兆网卡 | CentOS 7.5 | 192.168.0.16 |
| TiKV | 8 | 8 | SAS | 千兆网卡 | CentOS 7.5 | 192.168.0.15 |
| TiKV | 8 | 8 | SAS | 千兆网卡 | CentOS 7.5 | 192.168.0.14 |
| TiFlash | 4 | 8 | SAS | 千兆网卡 | CentOS 7.5 | 192.168.0.17 |

### TiDB 集群部署
1. 安装 TiUP 组件
```
// 安装 TiUP 工具
# useradd tidb
# su - tidb
$ curl --proto '=https' --tlsv1.2 -sSf https://tiup-mirrors.pingcap.com/install.sh | sh
$ source /home/tidb/.bash_profile
$ which tiup 
~/.tiup/bin/tiup

// 安装 TiUP cluster 组件
$ tiup cluster

// 更新 TiUP cluster 组件
$ tiup update --self && tiup update cluster

// 查看 TiUP cluster 版本信息
$ tiup --binary cluster
/home/tidb/.tiup/components/cluster/v1.0.9/tiup-cluster
```

2. 编辑初始化配置文件
```
$ vi topology.yaml
# # Global variables are applied to all deployments and used as the default value of
# # the deployments if a specific deployment value is missing.
global:
  user: "tidb"
  ssh_port: 22
  deploy_dir: "/tidb-deploy"
  data_dir: "/tidb-data"

# # Monitored variables are applied to all the machines.
monitored:
  node_exporter_port: 9100
  blackbox_exporter_port: 9115
  # deploy_dir: "/tidb-deploy/monitored-9100"
  # data_dir: "/tidb-data/monitored-9100"
  # log_dir: "/tidb-deploy/monitored-9100/log"

# # Server configs are used to specify the runtime configuration of TiDB components.
# # All configuration items can be found in TiDB docs:
# # - TiDB: https://pingcap.com/docs/stable/reference/configuration/tidb-server/configuration-file/
# # - TiKV: https://pingcap.com/docs/stable/reference/configuration/tikv-server/configuration-file/
# # - PD: https://pingcap.com/docs/stable/reference/configuration/pd-server/configuration-file/
# # All configuration items use points to represent the hierarchy, e.g:
# #   readpool.storage.use-unified-pool
# #      
# # You can overwrite this configuration via the instance-level `config` field.

server_configs:
  tidb:
    log.slow-threshold: 300
  tikv:
    # server.grpc-concurrency: 4
    # raftstore.apply-pool-size: 2
    # raftstore.store-pool-size: 2
    # rocksdb.max-sub-compactions: 1
    # storage.block-cache.capacity: "16GB"
    # readpool.unified.max-thread-count: 12
    readpool.storage.use-unified-pool: false
    readpool.coprocessor.use-unified-pool: true
  pd:
    schedule.leader-schedule-limit: 4
    schedule.region-schedule-limit: 2048
    schedule.replica-schedule-limit: 64
    replication.enable-placement-rules: true
  tiflash:
    # Maximum memory usage for processing a single query. Zero means unlimited.
    profiles.default.max_memory_usage: 10000000000
    # Maximum memory usage for processing all concurrently running queries on the server. Zero means unlimited.
    profiles.default.max_memory_usage_for_all_queries: 0

pd_servers:
  - host: 192.168.0.17
    # ssh_port: 22
    # name: "pd-1"
    # client_port: 2379
    # peer_port: 2380
    # deploy_dir: "/tidb-deploy/pd-2379"
    # data_dir: "/tidb-data/pd-2379"
    # log_dir: "/tidb-deploy/pd-2379/log"
    # numa_node: "0,1"
    # # The following configs are used to overwrite the `server_configs.pd` values.
    # config:
    #   schedule.max-merge-region-size: 20
    #   schedule.max-merge-region-keys: 200000
  - host: 192.168.0.18
  - host: 192.168.0.19
tidb_servers:
  - host: 192.168.0.19
    # ssh_port: 22
    # port: 4000
    # status_port: 10080
    # deploy_dir: "/tidb-deploy/tidb-4000"
    # log_dir: "/tidb-deploy/tidb-4000/log"
    # numa_node: "0,1"
    # # The following configs are used to overwrite the `server_configs.tidb` values.
    # config:
    #   log.slow-query-file: tidb-slow-overwrited.log
	# - host: 10.0.1.8
	# - host: 10.0.1.9
tikv_servers:
  - host: 192.168.0.14
    # ssh_port: 22
    # port: 20160
    # status_port: 20180
    # deploy_dir: "/tidb-deploy/tikv-20160"
    # data_dir: "/tidb-data/tikv-20160"
    # log_dir: "/tidb-deploy/tikv-20160/log"
    # numa_node: "0,1"
    # # The following configs are used to overwrite the `server_configs.tikv` values.
    # config:
    #   server.grpc-concurrency: 4
    #   server.labels: { zone: "zone1", dc: "dc1", host: "host1" }
  - host: 192.168.0.15
  - host: 192.168.0.16

tiflash_servers:
  - host: 192.168.0.17
    data_dir: /mtime/tidb/tidb-data/tiflash-9000
    deploy_dir: /mtime/tidb/tidb-deploy/tiflash-9000
    # ssh_port: 22
    # tcp_port: 9000
    # http_port: 8123
    # flash_service_port: 3930
    # flash_proxy_port: 20170
    # flash_proxy_status_port: 20292
    # metrics_port: 8234
    # deploy_dir: /tidb-deploy/tiflash-9000
    # numa_node: "0,1"
    # # The following configs are used to overwrite the `server_configs.tiflash` values.
    # config:
    #   logger.level: "info"
    # learner_config:
    #   log-level: "info"
    # - host: 10.0.1.12
    # - host: 10.0.1.13

monitoring_servers:
  - host: 192.168.0.18
    # ssh_port: 22
    # port: 9090
    # deploy_dir: "/tidb-deploy/prometheus-8249"
    # data_dir: "/tidb-data/prometheus-8249"
    # log_dir: "/tidb-deploy/prometheus-8249/log"

grafana_servers:
  - host: 192.168.0.18
    # port: 3000
    # deploy_dir: /tidb-deploy/grafana-3000

alertmanager_servers:
  - host: 192.168.0.18
    # ssh_port: 22
    # web_port: 9093
    # cluster_port: 9094
    # deploy_dir: "/tidb-deploy/alertmanager-9093"
    # data_dir: "/tidb-data/alertmanager-9093"
    # log_dir: "/tidb-deploy/alertmanager-9093/log"
```

3. 执行部署命令
```
$ tiup cluster deploy tidb-benchmark nightly ./topology.yaml --user root -p
```

4. 查看并启动 TiDB 集群
```
// 查看 TiUP 管理的集群情况
$ tiup cluster list
Starting component `cluster`: /home/tidb/.tiup/components/cluster/v1.0.9/tiup-cluster list
Name            User  Version  Path                                                      PrivateKey
----            ----  -------  ----                                                      ----------
tidb-benchmark  tidb  nightly  /home/tidb/.tiup/storage/cluster/clusters/tidb-benchmark  /home/tidb/.tiup/storage/cluster/clusters/tidb-benchmark/ssh/id_rsa

// 检查部署的 TiDB 集群情况
$ tiup cluster display tidb-benchmark

// 启动 TiDB 集群
$ tiup cluster start tidb-benchmark

// 验证 TiDB 集群状态
$ tiup cluster display tidb-benchmark

// 登录数据库
$ mysql -u root -h 192.168.0.19 -P 4000

// 访问 TiDB Dashboard
http://192.168.0.19:2379/dashboard

// 访问 Grafana 监控
http://192.168.0.18:3000/
```

### sysbench 性能测试
1. sysbench 工具安装
```
# yum -y install make automake libtool pkgconfig libaio-devel
# yum -y install mariadb-devel openssl-devel
# mkdir sysbench
# cd sysbench
# wget https://github.com/akopytov/sysbench/archive/1.0.20.tar.gz
# tar -zxvf 1.0.20.tar.gz 
# cd sysbench-1.0.20/
# ./autogen.sh 
# ./configure
# make -j
# make install
# sysbench --version
sysbench 1.0.20
```

2. sysbench 测试准备
- 创建数据库
```
# mysql -u root -h 192.168.0.19 -P 4000
mysql> create database sysbench;
```

- 创建配置文件
```
# vi config
mysql-host=192.168.0.19
mysql-port=4000
mysql-user=root
mysql-db=sysbench
time=600
threads=128
report-interval=10
db-driver=mysql
```

- 准备数据与预热
```
// 使用 sysbench 工具生产数据
# sysbench --config-file=config oltp_point_select --tables=32 --table-size=100000 prepare

// 数据预热与统计信息收集
SELECT COUNT(pad) FROM sbtest1 USE INDEX(k_1);
......
SELECT COUNT(pad) FROM sbtest32 USE INDEX(k_1);

ANALYZE TABLE sbtest1;
......
ANALYZE TABLE sbtest32;
```

3. sysbench 场景测试
- Point select 场景测试
```
sysbench --config-file=config oltp_point_select --tables=32 --table-size=100000 run
```

- Update index 场景测试
```
sysbench --config-file=config oltp_update_index --tables=32 --table-size=100000 run
```

- Read only 场景测试
```
sysbench --config-file=config oltp_read_only --tables=32 --table-size=100000 run
```

- Write only 场景测试
```
sysbench --config-file=config oltp_write_only --tables=32 --table-size=100000 run
```

- Read Write 场景测试
```
sysbench --config-file=config oltp_read_write --tables=32 --table-size=100000 run
```
